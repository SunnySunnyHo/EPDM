Note: This is, of course, approximate and just kind of what will actually be said.
Also, very much an early draft.

(First part depends somewhat on how previous group ends their thing)


So, now the question becomes, "when should I use which design?"

If you have a small number of effects and wish to thoroughly investigate their effect on each other, you can use a Full Factorial experiment design.

If you have a large number of factors, and can make do with a little less resolution in your experiments, Fractional Factorial Designs are just what you need.

If you have loads of factors you want to take into account, and don't really care about two-factor interactions, then Plackett and Burman are your guys.


Got that? Good. Now it's time to present you with something completely new, something which, in reality, will be more useful than all of these designs most of the time.
The primary problem with what we have done so far is that it can only take into account two levels for each of the factors. What if you want to do it for, say, three factors?
If a full factorial design for eight factors would require 256 runs, just imagine how many runs eight factors with three levels would require!
Just to rub the point in a little more, here is a graph. (do graph-thing)
Sure, fractional factorial and P&B experimental designs use common sense to cut down the number of experiments needed, but in most "real life" situations this just isn't enough.

Where common sense just isn't enough, one can always rely on maths!

SLIDE

Cue the wonders of Response Surface Methodology!

SLIDE

The basic principle behind Response Surface Methodology, or RSM as I will call it from now on, is to perform some experiments, and then extrapolate further assumed results from those observations.

The idea is to build an equation where the factors, and one or more polynomials of said factors, are assigned coefficients. This can be done with as high a level of polynomials as is needed, but generally that isn't too many. preferably only two.

This is all based on reducing, in a sense simplifying, mathematical models to a given form, shown here.

For example, take this equation. Graphed out it looks like this. This can be simplified to this form.
Side by side, we can see that they don't match up exactly, but it is close enough.
Take care though: the approximation is only done within a very small  area. If we zoom out from both graphs, they look like this, i.e., not that similar.
The moral is, make sure you are aware of the constraints you are working within, and stick to them. Otherwise, be prepared to face the consequences.

So, now we will have a look at different ways of generating this model, using Central Composite Designs.

These CCDs are based on the factorial experiment designs, and use some vector calculations to evolve the design matrix in a way which eventually allows it to carry out a linear regression, i.e. the mathematical approximation I just showed you. We don't need to know the details behind these calculations, we have software that does this for us, but even though we don't need to be too concerned with how the software does its thing, we should have an understanding of what it does.

An important part of what is done, in order to enable the software to perform a linear regression with a reduced number of experiments, is to make the experimental design rotatable. So, what does this mean?

First, imagine you have a 3-level experiment with two factors. A normal factorial experiment design would look like this, and it would give you data for the black spots in the picture. Here I call the lowest level for -1, the middle level for 0, and the high level for 1.

Now we add a third factor to this, giving the figure three dimensions. This shows the 27 results which could be acquired for a three level, three factor design experiment. We have a lot of data-points, that is great, however, as the number of factors increases...
Well... It's not practical.

Now lets look at what an alternative would look like.

Rather than running all of these experiments, we replace a bunch of them with something that we call axial points.

Here is the difference between the two-factor versions of the two designs.
The centre point as calculated as it was before, the corners are calculated as it was before, but instead of the points being calculated along the edges of the model, their X and Y values are increased in a way which means they are the same distance from the centre as the corner points.

This symmetry means that every performed experiment has the same precision, and therefore a model can be based on this.

In the case of only having two factors, this still requires us to run 9 experiments. But looking at the case for three factors, using this strategy has cut down the number of experiments from 27 to 15. As the numbers of factors increase, this difference increases dramatically, making this a practical way of reducing the number of experiments needed to acquire a suitable model.

What I have just shown you is actually the Central Composite Circumscribed, or CCC Design. This is one of the standard ways of generating an experimental design suitable for making a quadratic model from.

In order to visualise it better, here is a 3D model showing the location of the data points.

Now, to perform this experiment you would need access to 5 levels for each factor: the ones represented by -1, 0 and 1, but also the two levels located above 1, and below -1.
Sometimes getting access to levels above or below the previously defined minimum or maximum levels might be a problem.
There is therefore also an alternative to the CCC Design, the CCI design, the Central Composite Inscribed design.

As you can see, instead of choosing values beyond the extremes, bounded by the square, the corners are instead brought in.

Here is a 3D model illustrating this.

This can still be used to fit a full quadratic model, and does not exceed the bounds coded by 1 and -1. However, it does still require 5 available levels for each factor.
A potentially significant downside of this method is its failure to perform experiments in the outer corners, so the "extremes" aren't tested.

If, however, you can't get more than three levels for each factor, the design can be done using the Central Composite Faced design, which, compared to the original design, essentially just eliminates some points. The upside is that you can still base a quadratic model on this design, and it still requires less runs than what a full design would. The downside is that it isn't rotatable.
It is also not orthogonal, which is a property that has also been held by the other designs. And what does it mean for a design to be orthogonal?

An orthogonal design means that you can easily augment, i.e. improve your model with new data, while still using the old data. In a case where you would want to increase the accuracy of the model, you could run experiments etc.etc.

